{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12708765,"sourceType":"datasetVersion","datasetId":8032132}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip uninstall -y transformers\n%pip install -U \"transformers>=4.39,<5\" accelerate sentencepiece safetensors\n# 推奨の固定例:\n# pip install -U transformers==4.41.2 accelerate==0.33.0 sentencepiece==0.1.99 safetensors\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:26:00.973404Z","iopub.execute_input":"2025-08-28T14:26:00.973603Z","iopub.status.idle":"2025-08-28T14:27:33.672877Z","shell.execute_reply.started":"2025-08-28T14:26:00.973586Z","shell.execute_reply":"2025-08-28T14:27:33.671990Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers, torch\nprint(\"transformers:\", transformers.__version__)\nprint(\"torch:\", torch.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:27:33.676709Z","iopub.execute_input":"2025-08-28T14:27:33.676973Z","iopub.status.idle":"2025-08-28T14:27:37.834637Z","shell.execute_reply.started":"2025-08-28T14:27:33.676938Z","shell.execute_reply":"2025-08-28T14:27:37.833885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# DeBERTa-v3-Large\n#  - name 用 / desc 用 を別々に学習\n#  - train: OOF確率CSV,  test: 予測確率CSV を保存\n# =========================================================\nimport os\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\nimport re\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom scipy.special import softmax\nimport inspect\nfrom transformers import TrainingArguments\nfrom transformers.trainer_utils import EvalPrediction\n\nfrom transformers import (\n    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n    Trainer, TrainingArguments, DataCollatorWithPadding\n)\nimport inspect\nfrom transformers import TrainingArguments\nimport torch, os\nimport torch.nn as nn\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:27:37.836550Z","iopub.execute_input":"2025-08-28T14:27:37.837108Z","iopub.status.idle":"2025-08-28T14:28:00.650314Z","shell.execute_reply.started":"2025-08-28T14:27:37.837088Z","shell.execute_reply":"2025-08-28T14:28:00.649393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------- Config ----------------\n@dataclass\nclass CFG:\n    model_name: str = \"microsoft/deberta-v3-large\"\n    seed: int = 42\n    n_splits: int = 5\n    max_len_default: int = 512       # desc 向け\n    max_len_name: int = 128          # name 向け（短文）\n    lr: float = 2e-5\n    epochs: int = 4\n    train_bs: int = 4\n    eval_bs: int = 8\n    weight_decay: float = 0.01\n    warmup_ratio: float = 0.1\n    grad_accum: int = 2\n    out_root: str = \"deberta_v3_large_runs\"\n    target_col: str = \"final_status\"\n    fp16: bool = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:28:00.651326Z","iopub.execute_input":"2025-08-28T14:28:00.652520Z","iopub.status.idle":"2025-08-28T14:28:00.658658Z","shell.execute_reply.started":"2025-08-28T14:28:00.652457Z","shell.execute_reply":"2025-08-28T14:28:00.657876Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_PATH = \"/kaggle/input/mufg-dataset-2025/train.csv\" \nTEST_PATH  = \"/kaggle/input/mufg-dataset-2025/test.csv\"\n\nos.makedirs(CFG.out_root, exist_ok=True)\nnp.random.seed(CFG.seed)\ntorch.manual_seed(CFG.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:28:00.659539Z","iopub.execute_input":"2025-08-28T14:28:00.659969Z","iopub.status.idle":"2025-08-28T14:28:00.721250Z","shell.execute_reply.started":"2025-08-28T14:28:00.659929Z","shell.execute_reply":"2025-08-28T14:28:00.720542Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\n\ndef dataPreprocessing(x):\n    x = x.lower()\n    x = removeHTML(x)\n    x = re.sub(\"@\\w+\", '',x)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()\n    return x\n\n    \ncList = {\n    \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\"he's\": \"he is\",\"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\n    \"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\"isn't\": \"is not\",\n    \"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\"shan't've\": \"shall not have\",\"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\n    \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\"we've\": \"we have\",\"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\"what're\": \"what are\",\n    \"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\"where'd\": \"where did\",\"where's\": \"where is\",\n    \"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n    \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\"you're\": \"you are\",\"you've\": \"you have\"\n}\n\n# Function to expand contractions\ndef expand_contractions(text, cList):\n    pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in cList.keys()) + r')\\b')\n    def replace(match):\n        return cList[match.group(0)]\n    return pattern.sub(replace, text)\n\ndef prepro(df, vectorizer=None):\n    df['name'] = df['name'].fillna(\"\").astype(str)\n    df['desc'] = df['desc'].fillna(\"\").astype(str)\n    df['name'] = [dataPreprocessing(x) for x in df['name']]\n    df['desc'] = [dataPreprocessing(x) for x in df['desc']]\n    \n    df['name'] = df['name'].apply(lambda x: expand_contractions(x, cList))\n    df['desc'] = df['desc'].apply(lambda x: expand_contractions(x, cList))\n\n    kw = df[\"keywords\"].fillna(\"\").str.replace(\"-\", \" \")\n    df[\"desc\"] = (\"Keywords: \" + kw + \" | Description: \" + df[\"desc\"]).str.strip()\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:28:00.722165Z","iopub.execute_input":"2025-08-28T14:28:00.722411Z","iopub.status.idle":"2025-08-28T14:28:00.738085Z","shell.execute_reply.started":"2025-08-28T14:28:00.722391Z","shell.execute_reply":"2025-08-28T14:28:00.737515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _make_training_args(out_dir):\n    import inspect, os, torch\n    from transformers import TrainingArguments\n\n    def has(param_name):  # 引数が存在するかのユーティリティ\n        return param_name in inspect.signature(TrainingArguments.__init__).parameters\n\n    # --- 基本 ---\n    kwargs = dict(\n        output_dir=out_dir,\n        learning_rate=CFG.lr,\n        num_train_epochs=CFG.epochs,\n        per_device_train_batch_size=CFG.train_bs,\n        per_device_eval_batch_size=CFG.eval_bs,\n        gradient_accumulation_steps=CFG.grad_accum,\n        weight_decay=CFG.weight_decay,\n        warmup_ratio=CFG.warmup_ratio,\n        seed=CFG.seed,\n        fp16=bool(CFG.fp16 and torch.cuda.is_available()),\n        logging_steps=100,\n    )\n\n    # --- 評価・保存戦略（旧新どちらでも） ---\n    eval_key = \"evaluation_strategy\" if has(\"evaluation_strategy\") else (\"eval_strategy\" if has(\"eval_strategy\") else None)\n    if eval_key:\n        kwargs[eval_key] = \"epoch\"\n    if has(\"save_strategy\"):\n        kwargs[\"save_strategy\"] = \"epoch\"\n    if eval_key and has(\"save_strategy\") and has(\"load_best_model_at_end\"):\n        kwargs[\"load_best_model_at_end\"] = True\n    if has(\"metric_for_best_model\"):\n        kwargs[\"metric_for_best_model\"] = \"f1\"\n    if has(\"greater_is_better\"):\n        kwargs[\"greater_is_better\"] = True\n    if has(\"save_total_limit\"):\n        kwargs[\"save_total_limit\"] = 1\n    if has(\"report_to\"):\n        kwargs[\"report_to\"] = \"none\"\n    if has(\"logging_strategy\"):\n        kwargs[\"logging_strategy\"] = \"steps\"\n\n    # --- 高速化オプション（存在すれば適用） ---\n    if has(\"group_by_length\"):\n        kwargs[\"group_by_length\"] = True\n    if has(\"dataloader_num_workers\"):\n        kwargs[\"dataloader_num_workers\"] = max(1, min(4, (os.cpu_count() or 2) - 1))\n    if has(\"dataloader_pin_memory\"):\n        kwargs[\"dataloader_pin_memory\"] = True\n    if has(\"optim\"):\n        # PyTorch 2.x なら fused を試す。未対応環境では自動で通常版にフォールバック\n        kwargs[\"optim\"] = \"adamw_torch_fused\"\n\n    # --- bf16 自動（fp16を使わない＆Ampere以上） ---\n    if has(\"bf16\"):\n        is_ampere = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n        kwargs[\"bf16\"] = (not kwargs.get(\"fp16\", False)) and is_ampere\n\n    return TrainingArguments(**kwargs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:28:00.738783Z","iopub.execute_input":"2025-08-28T14:28:00.739193Z","iopub.status.idle":"2025-08-28T14:28:00.752937Z","shell.execute_reply.started":"2025-08-28T14:28:00.739157Z","shell.execute_reply":"2025-08-28T14:28:00.752176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback\ncallbacks = [EarlyStoppingCallback(early_stopping_patience=1,\n                                   early_stopping_threshold=1e-4)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:28:00.753870Z","iopub.execute_input":"2025-08-28T14:28:00.754147Z","iopub.status.idle":"2025-08-28T14:28:00.766701Z","shell.execute_reply.started":"2025-08-28T14:28:00.754130Z","shell.execute_reply":"2025-08-28T14:28:00.765964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 追加：事前encodeとインデックスで切るDataset\nclass EncodedSubset(Dataset):\n    def __init__(self, enc, idx, labels=None):\n        self.enc = enc\n        self.idx = np.asarray(idx)\n        self.labels = None if labels is None else labels[self.idx]\n    def __len__(self): return len(self.idx)\n    def __getitem__(self, i):\n        j = self.idx[i]\n        item = {k: torch.tensor(v[j]) for k, v in self.enc.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[i], dtype=torch.long)\n        return item\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:28:00.768889Z","iopub.execute_input":"2025-08-28T14:28:00.769103Z","iopub.status.idle":"2025-08-28T14:28:00.779685Z","shell.execute_reply.started":"2025-08-28T14:28:00.769088Z","shell.execute_reply":"2025-08-28T14:28:00.778913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def maybe_prefix_for_e5(texts, model_name: str):\n    \"\"\"e5系は 'passage: ' を前置（desc/name ともに文書=passage扱い）\"\"\"\n    if \"e5\" in model_name.lower():\n        return [f\"passage: {t}\" if isinstance(t, str) else \"passage: \" for t in texts]\n    return [t if isinstance(t, str) else \"\" for t in texts]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:28:00.780392Z","iopub.execute_input":"2025-08-28T14:28:00.780642Z","iopub.status.idle":"2025-08-28T14:28:00.787925Z","shell.execute_reply.started":"2025-08-28T14:28:00.780625Z","shell.execute_reply":"2025-08-28T14:28:00.787093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# helpers & config\n# =========================\nimport os, inspect, gc\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom scipy.special import softmax\nfrom tqdm.auto import tqdm\n\nfrom transformers import (\n    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n    DataCollatorWithPadding, TrainingArguments, Trainer,\n)\nfrom transformers.trainer_utils import EvalPrediction\n\n# 進捗の表示/抑制を一元管理\ndef _tqdm(seq, **kw):\n    return tqdm(seq, leave=False, ncols=100, **kw)\n\ndef maybe_prefix_for_e5(texts, model_name):\n    # e5 は \"passage: \" を前置（desc/長文は passage 扱い）\n    if \"e5\" in model_name.lower():\n        return [f\"passage: {t if isinstance(t, str) else ''}\" for t in texts]\n    return [t if isinstance(t, str) else \"\" for t in texts]\n\n# EncodedSubset: 事前トークナイズ済み辞書(enc)を index で取り出す Dataset\nclass EncodedSubset(torch.utils.data.Dataset):\n    def __init__(self, enc_dict, indices, labels):\n        self.enc = enc_dict\n        self.indices = np.array(indices)\n        self.labels = None if labels is None else np.array(labels)\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, i):\n        idx = self.indices[i]\n        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n# 重み付きCE（元コードの意図を踏襲）\nclass WeightedCETrainer(Trainer):\n    def __init__(self, class_weights=None, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        if class_weights is not None and not torch.is_tensor(class_weights):\n            class_weights = torch.tensor(class_weights, dtype=torch.float)\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs: bool = False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        if labels.dtype != torch.long:\n            labels = labels.to(torch.long)\n        outputs = model(**inputs)\n        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs.logits\n        num_labels = logits.size(-1)\n\n        weight = self.class_weights.to(logits.device) if self.class_weights is not None else None\n        ls = getattr(self.args, \"label_smoothing_factor\", 0.0)\n        loss_fct = nn.CrossEntropyLoss(weight=weight, label_smoothing=float(ls) if ls else 0.0)\n        loss = loss_fct(logits.view(-1, num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n\ndef compute_metrics(eval_pred):\n    if isinstance(eval_pred, EvalPrediction):\n        logits, labels = eval_pred.predictions, eval_pred.label_ids\n    else:\n        logits, labels = eval_pred\n    prob = softmax(logits, axis=1)[:, 1]\n    pred = (prob > 0.5).astype(int)\n    return {\"f1\": f1_score(labels, pred)}\n\n# TrainingArguments をここで作る（進捗が見える設定）\nimport inspect\nfrom transformers import TrainingArguments\n\ndef _make_training_args(output_dir,\n                        per_device_train_batch_size=16,\n                        per_device_eval_batch_size=64,\n                        num_train_epochs=1.0,\n                        fp16=True, bf16=False, seed=42):\n    \"\"\"\n    transformers のバージョン差異を吸収して TrainingArguments を構築\n    \"\"\"\n    sig = inspect.signature(TrainingArguments.__init__).parameters\n    kw = dict(\n        output_dir=output_dir,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=per_device_eval_batch_size,\n        num_train_epochs=num_train_epochs,\n        learning_rate=2e-5,\n        weight_decay=0.01,\n        gradient_accumulation_steps=1,\n        seed=seed,\n        logging_steps=50,\n    )\n\n    # あるものだけ入れる\n    if \"fp16\" in sig: kw[\"fp16\"] = fp16\n    if \"bf16\" in sig: kw[\"bf16\"] = bf16\n    if \"warmup_ratio\" in sig:\n        kw[\"warmup_ratio\"] = 0.06\n    elif \"warmup_steps\" in sig:\n        kw[\"warmup_steps\"] = 100  # 近似\n\n    # 評価/保存/ロギング系（新→旧の順で対応）\n    if \"evaluation_strategy\" in sig:\n        kw[\"evaluation_strategy\"] = \"steps\"\n        if \"eval_steps\" in sig: kw[\"eval_steps\"] = 200\n    elif \"evaluate_during_training\" in sig:\n        kw[\"evaluate_during_training\"] = True\n        if \"eval_steps\" in sig: kw[\"eval_steps\"] = 200\n\n    if \"logging_strategy\" in sig: kw[\"logging_strategy\"] = \"steps\"\n    if \"save_strategy\" in sig:    kw[\"save_strategy\"] = \"no\"\n    if \"report_to\" in sig:        kw[\"report_to\"] = \"none\"\n    if \"label_smoothing_factor\" in sig: kw[\"label_smoothing_factor\"] = 0.0\n    if \"dataloader_num_workers\" in sig: kw[\"dataloader_num_workers\"] = 2\n    if \"disable_tqdm\" in sig:     kw[\"disable_tqdm\"] = False\n    if \"load_best_model_at_end\" in sig: kw[\"load_best_model_at_end\"] = False\n\n    # None は渡さない\n    kw = {k: v for k, v in kw.items() if v is not None}\n    return TrainingArguments(**kw)\n\n# =========================\n# main function (差し替え)\n# =========================\ndef run_one_text_column(text_col: str, max_len: int, model_name: str | None = None):\n    if model_name is None:\n        model_name = CFG.model_name\n    print(f\"\\n========== RUN for column: {text_col} (max_len={max_len}, model={model_name}) ==========\")\n\n    # 出力先\n    safe_model_tag = model_name.replace(\"/\", \"_\").replace(\"-\", \"_\")\n    out_dir = os.path.join(CFG.out_root, f\"{text_col}__{safe_model_tag}\")\n    os.makedirs(out_dir, exist_ok=True)\n\n    # データ読み込み\n    df  = pd.read_csv(TRAIN_PATH)\n    dft = pd.read_csv(TEST_PATH)\n    df = prepro(df)\n    dft = prepro(dft)\n    if text_col not in df.columns or text_col not in dft.columns:\n        raise ValueError(f\"列 {text_col} が train/test の両方に存在しません。\")\n    df[CFG.target_col] = df[CFG.target_col].astype(int)\n    print(f\"Train size: {len(df)} | Test size: {len(dft)}\")\n\n    # Tokenizer / collator\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n\n    # テキスト前処理（e5は 'passage: ' を前置）\n    train_texts = maybe_prefix_for_e5(df[text_col].fillna(\"\").astype(str).tolist(), model_name)\n    test_texts  = maybe_prefix_for_e5(dft[text_col].fillna(\"\").astype(str).tolist(), model_name)\n\n    # 進捗表示しながらトークナイズ（大規模でも状況が見える）  ### NEW\n    def batch_tokenize(texts):\n        enc = {\"input_ids\": [], \"attention_mask\": []}\n        bs = 2048  # CPUバッチ\n        for i in _tqdm(range(0, len(texts), bs), desc=\"Tokenizing\"):\n            batch = texts[i:i+bs]\n            out = tokenizer(batch, truncation=True, max_length=max_len, padding=False)\n            enc[\"input_ids\"].extend(out[\"input_ids\"])\n            enc[\"attention_mask\"].extend(out[\"attention_mask\"])\n        return enc\n\n    enc_train = batch_tokenize(train_texts)\n    enc_test  = batch_tokenize(test_texts)\n\n    test_ds = EncodedSubset(enc_test, np.arange(len(dft)), None)\n\n    skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)\n    oof_prob = np.zeros(len(df), dtype=np.float32)\n    test_prob_folds = []\n\n    for fold, (trn_idx, val_idx) in enumerate(skf.split(df, df[CFG.target_col])):\n        print(f\"\\n---- Fold {fold+1}/{CFG.n_splits} ----\")\n        trn_df = df.iloc[trn_idx].reset_index(drop=True)\n        val_df = df.iloc[val_idx].reset_index(drop=True)\n\n        train_ds = EncodedSubset(enc_train, trn_idx, df[CFG.target_col].values)\n        valid_ds = EncodedSubset(enc_train, val_idx, df[CFG.target_col].values)\n\n        config = AutoConfig.from_pretrained(model_name)\n        config.num_labels = 2\n\n        model  = AutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n        # model.gradient_checkpointing_enable()  # 必要ならONに（VRAM節約）\n\n        # クラス重み\n        cw = compute_class_weight(\"balanced\", classes=[0, 1], y=trn_df[CFG.target_col].values)\n\n        # ★ 進捗/ログが見える TrainingArguments\n        args = _make_training_args(\n            os.path.join(out_dir, f\"fold{fold}\"),\n            per_device_train_batch_size=CFG.train_bs if hasattr(CFG, \"train_bs\") else 16,\n            per_device_eval_batch_size=CFG.valid_bs if hasattr(CFG, \"valid_bs\") else 64,\n            num_train_epochs=CFG.epochs if hasattr(CFG, \"epochs\") else 1.0,\n            fp16=torch.cuda.is_available(),  # 自動でfp16\n            bf16=False,\n            seed=CFG.seed\n        )\n\n        # HFのバージョン差異に対応（tokenizer/processing_class引数）\n        extra_init = {}\n        trainer_sig = inspect.signature(Trainer.__init__).parameters\n        if \"processing_class\" in trainer_sig:\n            extra_init[\"processing_class\"] = tokenizer\n        elif \"tokenizer\" in trainer_sig:\n            extra_init[\"tokenizer\"] = tokenizer\n\n        trainer = WeightedCETrainer(\n            class_weights=cw,\n            model=model,\n            args=args,\n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            data_collator=data_collator,\n            compute_metrics=compute_metrics,\n            **extra_init,\n        )\n\n        print(\"Training...\")\n        trainer.train()  # tqdmで進捗が出ます\n\n        # OOF\n        print(\"Predict (valid)...\")\n        logits_val = trainer.predict(valid_ds).predictions  # ここもtqdm表示あり\n        prob_val = softmax(logits_val, axis=1)[:, 1].astype(np.float32)\n        oof_prob[val_idx] = prob_val\n        f1_fold = f1_score(val_df[CFG.target_col].values, (prob_val > 0.5).astype(int))\n        print(f\"Fold {fold} F1 @0.5: {f1_fold:.4f}\")\n\n        # Test（foldごと）\n        print(\"Predict (test)...\")\n        logits_te = trainer.predict(test_ds).predictions\n        prob_te = softmax(logits_te, axis=1)[:, 1].astype(np.float32)\n        test_prob_folds.append(prob_te)\n\n        # メモリ解放\n        del trainer, model, train_ds, valid_ds, logits_val, logits_te\n        torch.cuda.empty_cache(); gc.collect()\n\n    # ------- 保存：train（OOF） -------\n    df_oof = pd.DataFrame({\n        \"id\": df[\"id\"] if \"id\" in df.columns else np.arange(len(df)),\n        f\"oof_prob_{text_col}\": oof_prob,\n        \"label\": df[CFG.target_col].values\n    })\n    oof_path = os.path.join(out_dir, f\"oof_{safe_model_tag}_{text_col}.csv\")\n    df_oof.to_csv(oof_path, index=False)\n\n    # ------- 保存：test（fold平均） -------\n    test_prob = np.mean(np.vstack(test_prob_folds), axis=0).astype(np.float32)\n    df_test_pred = pd.DataFrame({\n        \"id\": dft[\"id\"] if \"id\" in dft.columns else np.arange(len(dft)),\n        f\"prob_{text_col}\": test_prob\n    })\n    test_path = os.path.join(out_dir, f\"test_{safe_model_tag}_{text_col}.csv\")\n    df_test_pred.to_csv(test_path, index=False)\n\n    # レポート\n    ths = np.linspace(0.05, 0.95, 37)\n    f1s = [f1_score(df_oof[\"label\"], (df_oof[f\"oof_prob_{text_col}\"] > t).astype(int)) for t in ths]\n    best_t = ths[int(np.argmax(f1s))]\n    print(f\"\\n[{text_col}] OOF F1 @0.5 = {f1_score(df_oof['label'], (df_oof[f'oof_prob_{text_col}']>0.5).astype(int)):.4f}\")\n    print(f\"[{text_col}] OOF F1 @best={best_t:.2f} = {max(f1s):.4f}\")\n    print(f\"Saved:\\n - {oof_path}\\n - {test_path}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:28:00.789097Z","iopub.execute_input":"2025-08-28T14:28:00.789384Z","iopub.status.idle":"2025-08-28T14:28:00.822595Z","shell.execute_reply.started":"2025-08-28T14:28:00.789368Z","shell.execute_reply":"2025-08-28T14:28:00.821740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# とにかく速く：MiniLM-L6\nrun_one_text_column(\"desc\", max_len=256, model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\n# 速くて精度も良い：GTE small\n# run_one_text_column(\"desc\", max_len=256, model_name=\"thenlper/gte-small\")\n\n# e5 の小型版（前置きはそのまま適用されます）\n# run_one_text_column(\"desc\", max_len=256, model_name=\"intfloat/e5-small-v2\")\n\n# DeBERTa軽量\n# run_one_text_column(\"desc\", max_len=256, model_name=\"microsoft/deberta-v3-small\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:45:35.611585Z","iopub.execute_input":"2025-08-28T14:45:35.612430Z","iopub.status.idle":"2025-08-28T17:23:10.865976Z","shell.execute_reply.started":"2025-08-28T14:45:35.612398Z","shell.execute_reply":"2025-08-28T17:23:10.865025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) all-mpnet-base-v2\n# run_one_text_column(\"desc\", max_len=256, model_name=\"sentence-transformers/all-mpnet-base-v2\")\n\n# 2) e5-large-v2（自動で 'passage: ' が前置されます）\n# run_one_text_column(\"desc\", max_len=256, model_name=\"intfloat/e5-large-v2\")\n\n# 3) deberta-v3-large\n# run_one_text_column(\"desc\", max_len=256, model_name=\"microsoft/deberta-v3-large\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T14:32:47.094791Z","iopub.execute_input":"2025-08-28T14:32:47.095164Z","iopub.status.idle":"2025-08-28T14:34:01.390408Z","shell.execute_reply.started":"2025-08-28T14:32:47.095137Z","shell.execute_reply":"2025-08-28T14:34:01.389273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# run_one_text_column(\"name\", max_len=128, model_name=\"microsoft/MiniLM-L12-H384-uncased\")\n# run_one_text_column(\"desc\", max_len=256, model_name=\"microsoft/deberta-v3-base\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T22:43:53.049524Z","iopub.status.idle":"2025-08-27T22:43:53.049862Z","shell.execute_reply.started":"2025-08-27T22:43:53.049693Z","shell.execute_reply":"2025-08-27T22:43:53.049711Z"}},"outputs":[],"execution_count":null}]}